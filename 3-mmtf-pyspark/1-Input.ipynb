{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Structures\n",
    "mmtf-pyspark operates on 3D structures in the MMTF file format.\n",
    "\n",
    "Protein Data Bank structures are available in two MMTF data representations:\n",
    "* full\n",
    " * All atom representation \n",
    " * 0.001Å coordinate precision, 0.01 B-factor and occupancy precision\n",
    "* reduced\n",
    " * C-alpha atoms only for polypeptides \n",
    " * P-backbone atoms only for polynucleotides \n",
    " * All atom representation for all other residue types \n",
    " * 0.1Å coordinate precision, 0.1 B-factor and occupancy precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pyspark and mmtfPyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from mmtfPyspark.io import mmtfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"1-Input\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Structures\n",
    "For a small list of PDB entries (10s to 100), the download methods are the quickest way to import structures. Here we download a list of 4 structure in the full representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdbids = ['1LQ9','1LXJ','4XPX','1P1J']\n",
    "structures = mmtfReader.download_full_mmtf_files(pdbids, sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structures are represented as keyword-value pairs:\n",
    "* key: structure identifier (e.g., PDB ID)\n",
    "* value: MmtfStructure (structure data)\n",
    "\n",
    "We can print the keys and values using the collect() methods. Note, that the structures are loaded in an arbritray order. You cannot rely on the order of structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.73 ms, sys: 3.96 ms, total: 12.7 ms\n",
      "Wall time: 1.88 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1P1J', '1LQ9', '1LXJ', '4XPX']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "structures.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<mmtfPyspark.utils.mmtfStructure.MmtfStructure at 0x10fc64f28>,\n",
       " <mmtfPyspark.utils.mmtfStructure.MmtfStructure at 0x10fc64e80>,\n",
       " <mmtfPyspark.utils.mmtfStructure.MmtfStructure at 0x10fc8b240>,\n",
       " <mmtfPyspark.utils.mmtfStructure.MmtfStructure at 0x10fca6eb8>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures.values().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark represents these keyword-value pairs as Resilient Distributed Datasets (RDDs), which are a fault-tolerant collection of elements that can be operated on in parallel. To see how the dataset was distributed, we can print the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading structures from an MMTF Hadoop Sequence File\n",
    "Next, we read PDB structures from a local copy of an MMTF Hadoop Sequence file. For the following examples to work, the MMTF_FULL and MMTF_REDUCED environment variables need to be set. See installation instructions for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have long list (1000s) of PDB IDs, you can read the list of structures from a local copy of the MMTF Hadoop Sequence file,\n",
    "however, it's very inefficent for a few structures, e.g, in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop Sequence file path: MMTF_FULL=/Users/peter/MMTF_Files/full_pisces25_2.2_drugs\n"
     ]
    }
   ],
   "source": [
    "structures = mmtfReader.read_full_sequence_file(sc, pdbids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the keys again and see how long this takes. You can see that Spark loads the data only when and if it's required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.6 ms, sys: 3.07 ms, total: 9.67 ms\n",
      "Wall time: 4.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1LQ9', '1LXJ', '4XPX', '1P1J']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "structures.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read the entire PDB archive from the MMTF Hadoop Sequence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop Sequence file path: MMTF_FULL=/Users/peter/MMTF_Files/full_pisces25_2.2_drugs\n"
     ]
    }
   ],
   "source": [
    "structures = mmtfReader.read_full_sequence_file(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = mmtfReader.read_sequence_file(\"/Users/peter/MMTF_Files/reduced_pisces25_2.2_drugs\", sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.21 ms, sys: 3.96 ms, total: 11.2 ms\n",
      "Wall time: 17.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10707"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "structures.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of structures again. Should this be faster this time since we already loaded the entire PDB? \n",
    "\n",
    "No, the data from the Hadoop Sequence file are streamed through parallel threads. If you need the data again, in this case count again, the data need to be reloaded from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.41 ms, sys: 3.86 ms, total: 13.3 ms\n",
      "Wall time: 41.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10707"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "structures.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very Important: Stop Spark!!!\n",
    "It is very important to run the notebook all the way to the sc.stop() statement to terminate Spark. Otherwise you will endup running multiple instances of Spark that will interfere with each other. If necessary, kill any running Spark processes using the Activity Monitor on Mac or the Task Manager on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
